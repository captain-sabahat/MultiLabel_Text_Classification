{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow pyyaml torch transformers datasets gensim scikit-learn pandas numpy matplotlib seaborn nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, hamming_loss, precision_score, recall_score\n",
    "\n",
    "# Cell 3: Set Random Seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ PyTorch device: {device}\")\n",
    "\n",
    "# Cell 4: Load Configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Configuration loaded from config.yaml\")\n",
    "print(f\"✓ Experiment: {config['mlflow']['experiment_name']}\")\n",
    "\n",
    "# Cell 5: Initialize MLflow\n",
    "mlflow.set_tracking_uri(config['mlflow']['tracking_uri'])\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(config['mlflow']['experiment_name'])\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(config['mlflow']['experiment_name'])\n",
    "    print(f\"✓ Created MLflow experiment (ID: {experiment_id})\")\n",
    "else:\n",
    "    print(f\"✓ Using existing MLflow experiment (ID: {experiment.experiment_id})\")\n",
    "\n",
    "mlflow.set_experiment(config['mlflow']['experiment_name'])\n",
    "\n",
    "print(\"\\n✓ SNIPPET 1 COMPLETE: Setup & Configuration Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324baa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 2: Data Loading & Preprocessing\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATA LOADING & PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cell 1: Load Dataset\n",
    "print(\"\\nLoading GoEmotions dataset...\")\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "print(f\"Dataset splits available: {list(dataset.keys())}\")\n",
    "\n",
    "# Cell 2: Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text based on config\"\"\"\n",
    "    if config['preprocessing']['lowercase']:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if config['preprocessing']['remove_urls']:\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    if config['preprocessing']['remove_special_chars']:\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Cell 3: Load and Process Data\n",
    "def load_data(dataset_split, max_samples=None):\n",
    "    \"\"\"Load dataset split and convert to tokens + labels\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    tokens_list = []\n",
    "    \n",
    "    for i, example in enumerate(dataset_split):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        text = example['text']\n",
    "        texts.append(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = preprocess_text(text)\n",
    "        tokens_list.append(tokens)\n",
    "        \n",
    "        # Multi-label encoding: [0,0,1,0,...] where 1 = emotion present\n",
    "        label_vector = [0] * 28\n",
    "        for label_id in example['labels']:\n",
    "            label_vector[label_id] = 1\n",
    "        labels.append(label_vector)\n",
    "    \n",
    "    return texts, np.array(labels), tokens_list\n",
    "\n",
    "# Load splits\n",
    "train_texts, train_labels, train_tokens = load_data(\n",
    "    dataset['train'], \n",
    "    config['data']['train_size']\n",
    ")\n",
    "val_texts, val_labels, val_tokens = load_data(\n",
    "    dataset['validation'],\n",
    "    config['data']['val_size']\n",
    ")\n",
    "test_texts, test_labels, test_tokens = load_data(\n",
    "    dataset['test'],\n",
    "    config['data']['test_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loaded data splits:\")\n",
    "print(f\"  Train:  {len(train_texts)} samples\")\n",
    "print(f\"  Val:    {len(val_texts)} samples\")\n",
    "print(f\"  Test:   {len(test_texts)} samples\")\n",
    "print(f\"  Labels: {train_labels.shape} (multi-hot encoded)\")\n",
    "\n",
    "# Cell 4: Build Vocabulary\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_freq = Counter()\n",
    "for tokens in train_tokens:\n",
    "    word_freq.update(tokens)\n",
    "\n",
    "vocab = {word: idx + 2 for idx, (word, freq) in enumerate(word_freq.items()) if freq >= 2}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "print(f\"✓ Vocabulary size: {len(vocab)} tokens\")\n",
    "\n",
    "# Emotion labels reference\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval',\n",
    "    'caring', 'confusion', 'curiosity', 'desire', 'disappointment',\n",
    "    'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness',\n",
    "    'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "print(f\"✓ Emotion labels: {len(emotion_labels)} categories\")\n",
    "\n",
    "print(\"\\n✓ SNIPPET 2 COMPLETE: Data Loaded & Preprocessed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 3: Dense Embeddings Generation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: CREATE DENSE EMBEDDINGS FOR TEXT\")\n",
    "print(\"=\"*70)\n",
    "print(\"CRITICAL: Convert tokens to dense embedding vectors BEFORE neural network\")\n",
    "\n",
    "# Cell 1: Embedding Generator Class\n",
    "class DenseEmbeddingGenerator:\n",
    "    \"\"\"Generate dense embeddings from tokenized text\"\"\"\n",
    "    \n",
    "    def __init__(self, config, vocab, tokenized_texts):\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.embedding_dim = config['embeddings']['Word2Vec']['vector_size']\n",
    "    \n",
    "    def train_word2vec(self):\n",
    "        \"\"\"Train Word2Vec model\"\"\"\n",
    "        print(\"\\n[Word2Vec] Training embedding model...\")\n",
    "        cfg = self.config['embeddings']['Word2Vec']\n",
    "        \n",
    "        model = Word2Vec(\n",
    "            sentences=self.tokenized_texts,\n",
    "            vector_size=cfg['vector_size'],\n",
    "            window=cfg['window'],\n",
    "            min_count=cfg['min_count'],\n",
    "            sg=cfg['sg'],  # 1=Skip-gram\n",
    "            workers=cfg['workers'],\n",
    "            epochs=cfg['epochs']\n",
    "        )\n",
    "        \n",
    "        print(f\"  Vocabulary size: {len(model.wv)}\")\n",
    "        return model, cfg\n",
    "    \n",
    "    def train_glove(self):\n",
    "        \"\"\"Train GloVe-style (CBOW) model\"\"\"\n",
    "        print(\"\\n[GloVe] Training embedding model (CBOW)...\")\n",
    "        cfg = self.config['embeddings']['GloVe']\n",
    "        \n",
    "        model = Word2Vec(\n",
    "            sentences=self.tokenized_texts,\n",
    "            vector_size=cfg['vector_size'],\n",
    "            window=cfg['window'],\n",
    "            min_count=cfg['min_count'],\n",
    "            sg=cfg['sg'],  # 0=CBOW\n",
    "            workers=cfg['workers'],\n",
    "            epochs=cfg['epochs']\n",
    "        )\n",
    "        \n",
    "        print(f\"  Vocabulary size: {len(model.wv)}\")\n",
    "        return model, cfg\n",
    "    \n",
    "    def train_fasttext(self):\n",
    "        \"\"\"Train FastText model\"\"\"\n",
    "        print(\"\\n[FastText] Training embedding model...\")\n",
    "        cfg = self.config['embeddings']['FastText']\n",
    "        \n",
    "        model = FastText(\n",
    "            sentences=self.tokenized_texts,\n",
    "            vector_size=cfg['vector_size'],\n",
    "            window=cfg['window'],\n",
    "            min_count=cfg['min_count'],\n",
    "            sg=cfg['sg'],  # 1=Skip-gram\n",
    "            workers=cfg['workers'],\n",
    "            epochs=cfg['epochs']\n",
    "        )\n",
    "        \n",
    "        print(f\"  Vocabulary size: {len(model.wv)}\")\n",
    "        return model, cfg\n",
    "    \n",
    "    def text_to_dense_vector(self, tokens, embedding_model, method='mean'):\n",
    "        \"\"\"\n",
    "        CRITICAL: Convert token sequence to single dense vector\n",
    "        Methods: mean pooling, max pooling, sum pooling\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in embedding_model.wv:\n",
    "                vectors.append(embedding_model.wv[token])\n",
    "        \n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        vectors = np.array(vectors)\n",
    "        \n",
    "        if method == 'mean':\n",
    "            return np.mean(vectors, axis=0)\n",
    "        elif method == 'max':\n",
    "            return np.max(vectors, axis=0)\n",
    "        elif method == 'sum':\n",
    "            return np.sum(vectors, axis=0)\n",
    "    \n",
    "    def generate_dense_embeddings(self, texts_tokens, embedding_model, method='mean'):\n",
    "        \"\"\"Generate dense embeddings for all texts\"\"\"\n",
    "        dense_embeddings = []\n",
    "        \n",
    "        for tokens in tqdm(texts_tokens, desc=f\"Converting to dense vectors ({method})\"):\n",
    "            dense_vector = self.text_to_dense_vector(tokens, embedding_model, method)\n",
    "            dense_embeddings.append(dense_vector)\n",
    "        \n",
    "        return np.array(dense_embeddings)\n",
    "\n",
    "# Cell 2: Initialize Generator\n",
    "generator = DenseEmbeddingGenerator(config, vocab, train_tokens)\n",
    "\n",
    "# Cell 3: Generate Embeddings for Word2Vec, GloVe, FastText\n",
    "embeddings_data = {}\n",
    "\n",
    "if config['embeddings']['Word2Vec']['enabled']:\n",
    "    w2v_model, w2v_cfg = generator.train_word2vec()\n",
    "    train_dense_w2v = generator.generate_dense_embeddings(train_tokens, w2v_model, method='mean')\n",
    "    val_dense_w2v = generator.generate_dense_embeddings(val_tokens, w2v_model, method='mean')\n",
    "    test_dense_w2v = generator.generate_dense_embeddings(test_tokens, w2v_model, method='mean')\n",
    "    \n",
    "    embeddings_data['Word2Vec'] = {\n",
    "        'train': train_dense_w2v,\n",
    "        'val': val_dense_w2v,\n",
    "        'test': test_dense_w2v,\n",
    "        'config': w2v_cfg,\n",
    "        'dim': config['embeddings']['Word2Vec']['vector_size']\n",
    "    }\n",
    "    print(f\"  ✓ Dense embeddings shape: {train_dense_w2v.shape} (samples, dims)\")\n",
    "\n",
    "if config['embeddings']['GloVe']['enabled']:\n",
    "    glove_model, glove_cfg = generator.train_glove()\n",
    "    train_dense_glove = generator.generate_dense_embeddings(train_tokens, glove_model, method='mean')\n",
    "    val_dense_glove = generator.generate_dense_embeddings(val_tokens, glove_model, method='mean')\n",
    "    test_dense_glove = generator.generate_dense_embeddings(test_tokens, glove_model, method='mean')\n",
    "    \n",
    "    embeddings_data['GloVe'] = {\n",
    "        'train': train_dense_glove,\n",
    "        'val': val_dense_glove,\n",
    "        'test': test_dense_glove,\n",
    "        'config': glove_cfg,\n",
    "        'dim': config['embeddings']['GloVe']['vector_size']\n",
    "    }\n",
    "    print(f\"  ✓ Dense embeddings shape: {train_dense_glove.shape} (samples, dims)\")\n",
    "\n",
    "if config['embeddings']['FastText']['enabled']:\n",
    "    ft_model, ft_cfg = generator.train_fasttext()\n",
    "    train_dense_ft = generator.generate_dense_embeddings(train_tokens, ft_model, method='mean')\n",
    "    val_dense_ft = generator.generate_dense_embeddings(val_tokens, ft_model, method='mean')\n",
    "    test_dense_ft = generator.generate_dense_embeddings(test_tokens, ft_model, method='mean')\n",
    "    \n",
    "    embeddings_data['FastText'] = {\n",
    "        'train': train_dense_ft,\n",
    "        'val': val_dense_ft,\n",
    "        'test': test_dense_ft,\n",
    "        'config': ft_cfg,\n",
    "        'dim': config['embeddings']['FastText']['vector_size']\n",
    "    }\n",
    "    print(f\"  ✓ Dense embeddings shape: {train_dense_ft.shape} (samples, dims)\")\n",
    "\n",
    "# Cell 4: Generate BERT Embeddings\n",
    "if config['embeddings']['BERT']['enabled']:\n",
    "    print(\"\\n[BERT] Extracting dense embeddings...\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    bert_model.eval()\n",
    "    \n",
    "    def get_bert_embeddings(texts, tokenizer, model, batch_size=16):\n",
    "        \"\"\"Extract BERT [CLS] token embeddings (768-dim)\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"BERT embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                max_length=config['embeddings']['BERT']['max_length'],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = bert_model(input_ids, attention_mask)\n",
    "                embeddings = outputs.pooler_output.cpu().numpy()\n",
    "                all_embeddings.append(embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    train_dense_bert = get_bert_embeddings(train_texts, bert_tokenizer, bert_model)\n",
    "    val_dense_bert = get_bert_embeddings(val_texts, bert_tokenizer, bert_model)\n",
    "    test_dense_bert = get_bert_embeddings(test_texts, bert_tokenizer, bert_model)\n",
    "    \n",
    "    embeddings_data['BERT'] = {\n",
    "        'train': train_dense_bert,\n",
    "        'val': val_dense_bert,\n",
    "        'test': test_dense_bert,\n",
    "        'config': {'model_name': 'bert-base-uncased'},\n",
    "        'dim': 768  # BERT hidden size\n",
    "    }\n",
    "    print(f\"  ✓ Dense embeddings shape: {train_dense_bert.shape} (samples, dims)\")\n",
    "\n",
    "print(f\"\\n✓ STEP 2 COMPLETE: All texts converted to dense embeddings\")\n",
    "print(f\"✓ Total embedding models: {len(embeddings_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59043248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 4: Dataset Preparation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: CREATE DATASETS WITH DENSE EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cell 1: Dataset Class\n",
    "class DenseEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that takes DENSE embeddings as input\n",
    "    NOT tokens, NOT raw text - only fixed-size dense vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dense_embeddings, labels):\n",
    "        self.embeddings = torch.from_numpy(dense_embeddings).float()\n",
    "        self.labels = torch.from_numpy(labels).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embedding': self.embeddings[idx],      # (embedding_dim,)\n",
    "            'label': self.labels[idx]               # (28,)\n",
    "        }\n",
    "\n",
    "# Cell 2: Create Dataloaders\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "dataloaders = {}\n",
    "\n",
    "for embedding_name in embeddings_data.keys():\n",
    "    data = embeddings_data[embedding_name]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DenseEmbeddingDataset(data['train'], train_labels)\n",
    "    val_dataset = DenseEmbeddingDataset(data['val'], val_labels)\n",
    "    test_dataset = DenseEmbeddingDataset(data['test'], test_labels)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = config['training']['batch_size']\n",
    "    \n",
    "    dataloaders[embedding_name] = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "        'val': DataLoader(val_dataset, batch_size=batch_size),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size)\n",
    "    }\n",
    "    \n",
    "    train_batches = len(dataloaders[embedding_name]['train'])\n",
    "    val_batches = len(dataloaders[embedding_name]['val'])\n",
    "    test_batches = len(dataloaders[embedding_name]['test'])\n",
    "    \n",
    "    print(f\"  ✓ {embedding_name:12} - Train batches: {train_batches}, Val: {val_batches}, Test: {test_batches}\")\n",
    "\n",
    "print(f\"\\n✓ STEP 3 COMPLETE: Dataloaders Ready\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 5: Neural Network Architecture\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: BUILD NEURAL NETWORK FOR MULTI-LABEL CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Network input: Dense embeddings (fixed size)\")\n",
    "print(\"Network output: 28 logits (one per emotion label)\")\n",
    "\n",
    "# Cell 1: Neural Network Model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep neural network for multi-label classification\n",
    "    \n",
    "    STRICT REQUIREMENT:\n",
    "    Input: Dense embedding vectors (100-dim for W2V/GloVe/FastText, 768 for BERT)\n",
    "    Output: 28 logits (multi-label, one per emotion)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_labels, config):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        \n",
    "        # Get architecture from config\n",
    "        layers_dims = config['neural_network']['layers'].copy()\n",
    "        layers_dims[0] = input_dim  # Set input dimension dynamically\n",
    "        \n",
    "        print(f\"  Network architecture: {' -> '.join(map(str, layers_dims))}\")\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList() if config['neural_network']['batch_norm'] else None\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layers_dims) - 1):\n",
    "            self.layers.append(nn.Linear(layers_dims[i], layers_dims[i+1]))\n",
    "            \n",
    "            # Batch norm for hidden layers only\n",
    "            if config['neural_network']['batch_norm'] and i < len(layers_dims) - 2:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(layers_dims[i+1]))\n",
    "            \n",
    "            # Dropout for hidden layers only\n",
    "            if i < len(layers_dims) - 2:\n",
    "                self.dropouts.append(nn.Dropout(config['neural_network']['dropout']))\n",
    "        \n",
    "        self.activation = nn.ReLU() if config['neural_network']['activation'] == 'relu' else nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        x: (batch_size, embedding_dim) - DENSE EMBEDDINGS\n",
    "        output: (batch_size, 28) - logits\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers[:-1]):  # All except last\n",
    "            x = layer(x)\n",
    "            \n",
    "            if self.batch_norms is not None:\n",
    "                x = self.batch_norms[i](x)\n",
    "            \n",
    "            x = self.activation(x)\n",
    "            x = self.dropouts[i](x)\n",
    "        \n",
    "        # Output layer (no activation, no dropout)\n",
    "        x = self.layers[-1](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Cell 2: Display Network Architecture\n",
    "print(\"\\n✓ Network Architecture:\")\n",
    "print(f\"  Config layers: {config['neural_network']['layers']}\")\n",
    "print(f\"  Activation: {config['neural_network']['activation']}\")\n",
    "print(f\"  Dropout: {config['neural_network']['dropout']}\")\n",
    "print(f\"  Batch norm: {config['neural_network']['batch_norm']}\")\n",
    "print(f\"  Output: 28 logits (multi-label)\")\n",
    "print(f\"  Loss function: BCEWithLogitsLoss\")\n",
    "\n",
    "print(f\"\\n✓ STEP 4 COMPLETE: Neural Network Architecture Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe74cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 6: Training & Evaluation Functions\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEFINING TRAINING & EVALUATION FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cell 1: Training Function\n",
    "def train_model(model, train_loader, optimizer, config, device):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        embeddings = batch['embedding'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass: embeddings -> network -> logits\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Cell 2: Evaluation Function\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set\n",
    "    Compute: loss, F1, precision, recall, hamming loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            embeddings = batch['embedding'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Apply sigmoid and threshold\n",
    "            preds = torch.sigmoid(outputs) > threshold\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Stack all predictions and labels\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'hamming_loss': hamming_loss(all_labels, all_preds),\n",
    "        'micro_f1': f1_score(all_labels, all_preds, average='micro', zero_division=0),\n",
    "        'macro_f1': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n",
    "        'micro_precision': precision_score(all_labels, all_preds, average='micro', zero_division=0),\n",
    "        'micro_recall': recall_score(all_labels, all_preds, average='micro', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✓ Functions defined:\")\n",
    "print(\"  - train_model(): Train for one epoch\")\n",
    "print(\"  - evaluate_model(): Compute all metrics\")\n",
    "print(f\"\\n✓ SNIPPET 6 COMPLETE: Training Functions Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 7: Training Loop with MLflow Tracking\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: TRAIN & COMPARE ALL MODELS WITH MLFLOW TRACKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cell 1: Training Loop for All Embeddings\n",
    "all_results = {}\n",
    "training_histories = {}\n",
    "\n",
    "for embedding_name in embeddings_data.keys():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training with {embedding_name} Embeddings\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Start MLflow run\n",
    "    run_name = f\"{embedding_name}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "    \n",
    "    try:\n",
    "        # Cell 2: Log Parameters to MLflow\n",
    "        print(\"\\nLogging parameters to MLflow...\")\n",
    "        \n",
    "        mlflow.log_param('embedding_model', embedding_name)\n",
    "        mlflow.log_param('embedding_dim', embeddings_data[embedding_name]['dim'])\n",
    "        mlflow.log_param('num_labels', 28)\n",
    "        mlflow.log_param('num_epochs', config['training']['num_epochs'])\n",
    "        mlflow.log_param('batch_size', config['training']['batch_size'])\n",
    "        mlflow.log_param('learning_rate', config['training']['learning_rate'])\n",
    "        mlflow.log_param('dropout', config['neural_network']['dropout'])\n",
    "        mlflow.log_param('batch_norm', config['neural_network']['batch_norm'])\n",
    "        \n",
    "        # Log embedding-specific config\n",
    "        for key, value in embeddings_data[embedding_name]['config'].items():\n",
    "            mlflow.log_param(f'emb_{key}', value)\n",
    "        \n",
    "        # Cell 3: Create Model\n",
    "        input_dim = embeddings_data[embedding_name]['dim']\n",
    "        \n",
    "        model = MultiLabelClassifier(\n",
    "            input_dim=input_dim,\n",
    "            num_labels=28,\n",
    "            config=config\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"✓ Model created with input_dim={input_dim}\")\n",
    "        \n",
    "        # Cell 4: Setup Optimizer\n",
    "        learning_rate = config['training']['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Cell 5: Get Dataloaders\n",
    "        train_loader = dataloaders[embedding_name]['train']\n",
    "        val_loader = dataloaders[embedding_name]['val']\n",
    "        test_loader = dataloaders[embedding_name]['test']\n",
    "        \n",
    "        # Cell 6: Training Loop with Early Stopping\n",
    "        best_val_f1 = 0\n",
    "        patience = 0\n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "        \n",
    "        for epoch in range(config['training']['num_epochs']):\n",
    "            print(f\"\\nEpoch {epoch+1}/{config['training']['num_epochs']}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = train_model(model, train_loader, optimizer, config, device)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = evaluate_model(model, val_loader, device)\n",
    "            \n",
    "            # Store history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_metrics['loss'])\n",
    "            history['val_f1'].append(val_metrics['micro_f1'])\n",
    "            \n",
    "            # Log to MLflow\n",
    "            mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "            mlflow.log_metric('val_loss', val_metrics['loss'], step=epoch)\n",
    "            mlflow.log_metric('val_micro_f1', val_metrics['micro_f1'], step=epoch)\n",
    "            mlflow.log_metric('val_macro_f1', val_metrics['macro_f1'], step=epoch)\n",
    "            \n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_metrics['loss']:.4f}, Micro-F1: {val_metrics['micro_f1']:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_metrics['micro_f1'] > best_val_f1:\n",
    "                best_val_f1 = val_metrics['micro_f1']\n",
    "                patience = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"  ✓ Best model saved (F1: {best_val_f1:.4f})\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= config['training']['early_stopping_patience']:\n",
    "                    print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Cell 7: Load Best Model & Test\n",
    "        print(\"\\nEvaluating best model on test set...\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        test_metrics = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "        print(f\"\\n[{embedding_name}] TEST RESULTS:\")\n",
    "        for metric_name, metric_value in test_metrics.items():\n",
    "            print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "            mlflow.log_metric(f'test_{metric_name}', metric_value)\n",
    "        \n",
    "        all_results[embedding_name] = test_metrics\n",
    "        training_histories[embedding_name] = history\n",
    "        \n",
    "        # Cell 8: Save Model Artifact\n",
    "        model_path = f'{embedding_name}_best_model.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        mlflow.log_artifact(model_path)\n",
    "        print(f\"  ✓ Model saved: {model_path}\")\n",
    "        \n",
    "        mlflow.end_run()\n",
    "        print(f\"\\n✓ {embedding_name} training complete - MLflow run ended\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error training {embedding_name}: {e}\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "print(f\"\\n✓ SNIPPET 7 COMPLETE: All models trained & logged to MLflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 8: Results Comparison & Visualization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cell 1: Create Results DataFrame\n",
    "print(\"\\nCompiling results...\")\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
    "\n",
    "print(\"\\n✓ Model Performance Comparison:\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Cell 2: Save Results to CSV\n",
    "results_df.to_csv('embedding_comparison_final.csv')\n",
    "print(f\"\\n✓ Results saved to: embedding_comparison_final.csv\")\n",
    "\n",
    "# Cell 3: Create Comparison Visualizations\n",
    "print(\"\\nGenerating comparison plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "models = list(all_results.keys())\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Plot 1: Test Loss\n",
    "axes[0, 0].bar(models, [all_results[m]['loss'] for m in models], color=colors)\n",
    "axes[0, 0].set_title('Test Loss (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    height = all_results[m]['loss']\n",
    "    axes[0, 0].text(i, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Hamming Loss\n",
    "axes[0, 1].bar(models, [all_results[m]['hamming_loss'] for m in models], color=colors)\n",
    "axes[0, 1].set_title('Hamming Loss (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Hamming Loss')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    height = all_results[m]['hamming_loss']\n",
    "    axes[0, 1].text(i, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Micro-F1 Score\n",
    "axes[1, 0].bar(models, [all_results[m]['micro_f1'] for m in models], color=colors)\n",
    "axes[1, 0].set_title('Micro-F1 Score (Higher is Better)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Micro-F1')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    height = all_results[m]['micro_f1']\n",
    "    axes[1, 0].text(i, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Macro-F1 Score\n",
    "axes[1, 1].bar(models, [all_results[m]['macro_f1'] for m in models], color=colors)\n",
    "axes[1, 1].set_title('Macro-F1 Score (Higher is Better)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Macro-F1')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    height = all_results[m]['macro_f1']\n",
    "    axes[1, 1].text(i, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot = 'embedding_comparison_final.png'\n",
    "plt.savefig(comparison_plot, dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Comparison plot saved: {comparison_plot}\")\n",
    "\n",
    "# Cell 4: Log Final Comparison to MLflow\n",
    "mlflow.start_run(run_name='final_comparison')\n",
    "mlflow.log_artifact(comparison_plot)\n",
    "mlflow.log_artifact('embedding_comparison_final.csv')\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"✓ Final comparison logged to MLflow\")\n",
    "\n",
    "# Cell 5: Summary Statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Best Model by Micro-F1 Score:\")\n",
    "best_model = results_df.loc[results_df['micro_f1'].idxmax()]\n",
    "print(f\"   {best_model.name} with Micro-F1: {best_model['micro_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Best Model by Macro-F1 Score:\")\n",
    "best_macro_model = results_df.loc[results_df['macro_f1'].idxmax()]\n",
    "print(f\"   {best_macro_model.name} with Macro-F1: {best_macro_model['macro_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n3. Lowest Hamming Loss:\")\n",
    "best_hamming_model = results_df.loc[results_df['hamming_loss'].idxmin()]\n",
    "print(f\"   {best_hamming_model.name} with Hamming Loss: {best_hamming_model['hamming_loss']:.4f}\")\n",
    "\n",
    "print(\"\\n4. Model Rankings (by Micro-F1):\")\n",
    "ranked = results_df.sort_values('micro_f1', ascending=False).reset_index()\n",
    "for idx, row in ranked.iterrows():\n",
    "    print(f\"   {idx+1}. {row['index']:12} - Micro-F1: {row['micro_f1']:.4f}, Macro-F1: {row['macro_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ SNIPPET 8 COMPLETE: Comparison Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 9: MLflow Tracking Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MLFLOW EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cell 1: Get Experiment Info\n",
    "print(f\"\\nTracking URI: {config['mlflow']['tracking_uri']}\")\n",
    "print(f\"Experiment: {config['mlflow']['experiment_name']}\")\n",
    "\n",
    "# Cell 2: Search Runs\n",
    "print(\"\\nSearching all runs...\")\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[config['mlflow']['experiment_name']]\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Total runs: {len(runs)}\")\n",
    "\n",
    "# Cell 3: Display Run Summary\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"RUN SUMMARY:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for idx, run in runs.iterrows():\n",
    "    if 'final_comparison' not in run['run_name']:\n",
    "        print(f\"\\nRun {idx+1}: {run['run_name']}\")\n",
    "        print(f\"  Status: {run['status']}\")\n",
    "        print(f\"  Duration: {run['duration']/1000:.2f}s\")\n",
    "        \n",
    "        # Get embedding model from tags\n",
    "        embedding_model = run.get('tags.embedding_model', 'N/A')\n",
    "        print(f\"  Embedding: {embedding_model}\")\n",
    "        \n",
    "        # Get best test metrics if available\n",
    "        if 'test_micro_f1' in runs.columns:\n",
    "            test_f1 = run.get('metrics.test_micro_f1', 'N/A')\n",
    "            if test_f1 != 'N/A':\n",
    "                print(f\"  Test Micro-F1: {test_f1:.4f}\")\n",
    "\n",
    "# Cell 4: MLflow UI Command\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TO VIEW MLflow UI:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRun command:\")\n",
    "print(f\"  mlflow ui --backend-store-uri {config['mlflow']['tracking_uri']}\")\n",
    "print(f\"\\nThen open: http://localhost:5000\")\n",
    "\n",
    "print(\"\\n✓ SNIPPET 9 COMPLETE: MLflow Summary Ready\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
